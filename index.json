[{"content":"前言 繼上一篇[技術雜談] 你不能不知道的軟體架構\u0026ndash;MapReduce (一)提到的MapReduce概念，這篇講解的是如何完成MIT 6.824的Lab1，自幹一個MapReduce核心。雖然網路上許多人說這個 Lab不難，但其實我花了不少的時間(汗)，而且因為是由golang完成的，對goroutine與channel又必須有一些了解，所以我認為沒學過go的人要完成是蠻有挑戰的，也因為這樣，以下的軟體結構我盡量以圖來說明，程式碼的部分瀏覽即可。\n題目要求 首先我們再回顧一下論文中的圖：\n  mapReduce in paper\n  我將Lab1中的幾個重點列出，由於並非全部要求，若想完成Lab還是需要看原網址的描述。\n 設計一個分散式的MapReduce系統，其中包含兩個程式：Coordinator和Worker。 系統架構中只有一個Coordinator，一或多個Worker平行運行。 兩者之間的溝通透過RPC (Remote Procedure Call)。 Map階段將中介的key (前篇提到的k2)利用hash分成nReduce組。 第X組的Reduce任務輸出命名為mr-out-X。 中介檔案 (Map的產物)命名為mr-X-Y。 主程序會隔三差五的呼叫Done確認任務是否全數完成。  系統架構設計 根據以上幾點要求，我的想法是：\n 用一個會block的queue (channel in golang)作為任務佇列。 將任務分為兩階段：Map和Reduce，當所有任務的Map階段完成後，在最新一次Done被呼叫時更新狀態為Reduce階段。 Worker不停向Master (Coordinator)要求任務，並更新自身狀態  設計上如下圖：\n  my mapReduce design\n  基本上如果只是要理解設計的話到這就結束了，接下來會談在程式碼上如何實作。我們先從Coordinator和Worker的主程式開始看起：\n// main/mrcoordinator.go func main() { if len(os.Args) \u0026lt; 2 { fmt.Fprintf(os.Stderr, \u0026#34;Usage: mrcoordinator inputfiles...\\n\u0026#34;) os.Exit(1) } // 第一個參數是*.txt，代表切分後的文件 \t// 第二個參數則是worker數量(nReduce) \tm := mr.MakeCoordinator(os.Args[1:], 10) for m.Done() == false { time.Sleep(time.Second) } time.Sleep(time.Second) } // main/mrworker.go  func main() { if len(os.Args) != 2 { fmt.Fprintf(os.Stderr, \u0026#34;Usage: mrworker xxx.so\\n\u0026#34;) os.Exit(1) } mapf, reducef := loadPlugin(os.Args[1]) mr.Worker(mapf, reducef) } 可以從上面的程式看到Coordinator啟動後會每過一秒確認一次任務完成了嗎，而這個Lab還有一個要求是測試的程式mian/test-mr.sh要在10s內完成。而下方的Worker很簡單，就是單純的把mapf和reducef傳入即可。接者我們看看Coordinator本身的設計：\ntype Coordinator struct { TaskChan chan Task // 任務等待的quque \tFiles []string // 要處理的文件 \tMapNum int // nMap, 要處理的map任務數量，其實就是files數量 \tReduceNum int // nReduce, reduce任務數量 \tTaskPhase int // 0: mapPhase, 1: reducePhase \tTaskState []TaskState // 任務狀態 \tMutex sync.Mutex // 互斥鎖, 以防coordinator的狀態被不同worker同時讀寫 \tIsDone bool //任務是否全數完成 } 簡單來說，由於Coordinator要處理讀取任務(input)和Worker請求(output)兩者，就必須兼具分派任務且維持本身資訊穩定的特性，也因此需要Mutex來處理。更細部的處理，例如Task和TaskState的設計可以到我的專案看，這裡就不贅述，接著看Worker的設計：\nfunc Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { for { reply := requestTask() // 只有在任務全數完成時，回傳會是true \tif reply.TaskDone { break } err := do(mapf, reducef, reply.Task) if err != nil { reportTask(reply.Task.TaskIndex, false) // 回傳任務, 如果任務出錯了, arg2給予false \tcontinue } reportTask(reply.Task.TaskIndex, true) } } 從主程序收到mapf及reducef後，worker就知道自己確切要做什麼了，因此流程很簡單：\n 請求任務。 執行任務 (do()中會處理任務型態為map或reduce)。 根據成功或失敗回報任務。 重複1~3直到任務都被完成。  如此一來每個worker都充分利用到，且可以處理執行錯誤的任務。\n結論 這裡會發現，許多concurrency的架構都採用類似 fan-in/fan-out的模型，意思就是由一個master處理分派工作和蒐集，並做到block等待的工作，第一次讀到這個模型是在Effective Python這本書中提到，其中包含了許多關於python設計中GIL (全域鎖) 的用途及multi-thread使用方式，如對分散系統有興趣非常推薦一試。6.824的第一個lab到此結束，下一個lab是關於分散式系統中的共識機制演算法Raft，對於Cloud Native越趨熱門的時代，了解這些架構及演算法對建構雲生態相當有幫助。\n心得 與其說是講解，不如說是在疫情期間為自己紀錄學習歷程，最近對分散式系統越發著迷，標題寫著你不能不知道...，但其實潛台詞是在責怪我自己，明明想成為SRE，知道blockchain、用過nosql、學過data science，卻不知道他們每一個都和分散式系統密不可分，尤其blockchain本身就是一個分散式系統 (在6.824後段課程會講解)，想想還是應把基礎慢慢不足，現階段得趕快知道我必須讀什麼，當兵才不會太無聊。之後想寫一些關於github action如何部署hugo的網頁 (也就是此部落格的方式)。再次感謝看到這裡的你，祝你平安。\n","permalink":"http://nathan-tw.github.io/posts/mapreduce2/","summary":"前言 繼上一篇[技術雜談] 你不能不知道的軟體架構\u0026ndash;MapReduce (一)提到的MapReduce概念，這篇講解的是如何完成MIT 6.824的Lab1，自幹一個MapReduce核心。雖然網路上許多人說這個 Lab不難，但其實我花了不少的時間(汗)，而且因為是由golang完成的，對goroutine與channel又必須有一些了解，所以我認為沒學過go的人要完成是蠻有挑戰的，也因為這樣，以下的軟體結構我盡量以圖來說明，程式碼的部分瀏覽即可。\n題目要求 首先我們再回顧一下論文中的圖：\n  mapReduce in paper\n  我將Lab1中的幾個重點列出，由於並非全部要求，若想完成Lab還是需要看原網址的描述。\n 設計一個分散式的MapReduce系統，其中包含兩個程式：Coordinator和Worker。 系統架構中只有一個Coordinator，一或多個Worker平行運行。 兩者之間的溝通透過RPC (Remote Procedure Call)。 Map階段將中介的key (前篇提到的k2)利用hash分成nReduce組。 第X組的Reduce任務輸出命名為mr-out-X。 中介檔案 (Map的產物)命名為mr-X-Y。 主程序會隔三差五的呼叫Done確認任務是否全數完成。  系統架構設計 根據以上幾點要求，我的想法是：\n 用一個會block的queue (channel in golang)作為任務佇列。 將任務分為兩階段：Map和Reduce，當所有任務的Map階段完成後，在最新一次Done被呼叫時更新狀態為Reduce階段。 Worker不停向Master (Coordinator)要求任務，並更新自身狀態  設計上如下圖：\n  my mapReduce design\n  基本上如果只是要理解設計的話到這就結束了，接下來會談在程式碼上如何實作。我們先從Coordinator和Worker的主程式開始看起：\n// main/mrcoordinator.go func main() { if len(os.Args) \u0026lt; 2 { fmt.Fprintf(os.Stderr, \u0026#34;Usage: mrcoordinator inputfiles...\\n\u0026#34;) os.Exit(1) } // 第一個參數是*.txt，代表切分後的文件 \t// 第二個參數則是worker數量(nReduce) \tm := mr.","title":"[技術雜談] 你不能不知道的軟體架構--MapReduce (二)"},{"content":"前言 你肯定聽過大數據，甚至學會許多處理資料的方式，但在資訊爆炸的時代，企業要處理如此大量的資料通常不是依靠我們手上的小小筆電，那假如我們有了很多台機器然後呢？有什麼辦法是集結眾多機器的力量加速任務處理的呢？最近疫情嚴重在家讀MIT 6.824，對Google的AI大神Jeff Dean提出的MapReduce又認識了許多，之後許多知名的分散運算叢集都是基於這個概念，例如大家耳熟能詳的Hadoop和Spark，本篇文章將帶你深入淺出何謂MapReduce，下一篇則是以Go語言自幹一個MapReduce核心 ，有興趣可以先去我的github repo看。\n什麼是MapReduce 我們先想像有一個任務是要算出一篇很長的文章中字詞出現頻率，如果在同一台電腦我們會怎麼做呢？我們也許會遍歷整篇文章，並將字詞做紀錄，但若資料太大時這樣的方式並不可行，為了加速我們將檔案切分，並且產為經由Map function 產生 key-value pair如下：\nhello world -\u0026gt; {hello:1, world:1}\n接著將這些key-value pair 存成中介檔案後排序，Reduce function則根據key將所有value加總，如果以上流程在一台電腦上完成，會像下圖，前兩列中的A、B、C其實分別是Ａ:1、 B:1、C:1：\n  mapreduce in sequence, src: https://zhuanlan.zhihu.com/p/260752052\n  上面提到的任務如果在一個分散式的世界呢？Google提出的論文中有張重要的圖是這樣描述的：\n  mapreduce in distributed systems\n  MapReduce是一種軟體架構，由許多台機器組成，其中一台擔任Master，負責分派任務和處理來自Worker的RPC請求，其他則擔任Worker，負責處理使用者定義的Map和Reduce函式 也就是說只有可切分的大型任務才能應用MapReduce。而上面提到的例子如果以分散式系統實現，就會如下圖：\n  mapreduce in distributed systems, src: https://zhuanlan.zhihu.com/p/260752052\n  整個概念有點類似前陣子完成的Worker Pool，但是將thread改成分散於各個系統的Process。機器越多，系統擴展越困難，而MapReduce優雅的解決了這個問題，為了套用於許多不同的任務型態，mapreduce核心必須定義一致性的接口，這裏我們可以看到論文2.2提到的：\nmap (k1, v1) → list(k2, v2) reduce (k2, list(v2)) → list(v2) 在任務經過切分後，每個任務會交由一個mapper處理，而分配的機制則是誰有空誰就去處理(queue)。在word count的例子中，(k1, v1)代表切分後檔案名稱及內容，就是{filename, content}，而(k2, v2)則代表詞與頻率，也就是{hello: 1}。至於k2還有另一個用處，就是決定他要由哪個reducer處理，在產出中介檔案時就會依照mr-x-y命名，其中x代表map函數的index，y代表k2經過hash的值，這個值也同時是reducer的index。好的我知道許多人到這裡可能完全混亂，以下我用一張圖說明：\n  map function works in detail\n  可以清楚的看到切分後的小任務分別被機器執行，其實他們是在一個queue中等待分發，但這裡為了顯示不同檔案對應的流程才這樣畫，另一方面是想表示一個重要的概念，只有能夠切分的任務才能使用mapReduce。\n結論 以下整理幾個重點：\n 在分散式的世界中，效率是一致性的敵人(這裡的一致性指的是各個機器間的狀態)。 master負責分配任務，而任務分成map及reduce兩類。 mapReduce核心負責歸類和排序，使用者則定義map和reduce確切要做的事。 只有能夠切分的任務才能使用mapReduce。  心得 疫情肆虐，在家的各位好嗎？也許很多人的生涯規劃被打亂，期待許久的聚餐也迫於無奈取消，旅遊或生活都淪為遺憾。但在家依然有許多事可以做的，例如寫一篇廢廢的部落格聊聊近日所學，下一篇我會談談最近在看的課程MIT6.824中lab1的實作，著墨較多於系統設計，如果看不懂go也沒關係。 感謝你看到這裏，願你平安。\nReference  MapReduce Paper: https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf Hadoop: https://en.wikipedia.org/wiki/Apache_Hadoop Spark: https://en.wikipedia.org/wiki/Apache_Spark 知乎講解: https://zhuanlan.zhihu.com/p/260752052 我的Lab1: https://github.com/nathan-tw/6.824  ","permalink":"http://nathan-tw.github.io/posts/mapreduce/","summary":"前言 你肯定聽過大數據，甚至學會許多處理資料的方式，但在資訊爆炸的時代，企業要處理如此大量的資料通常不是依靠我們手上的小小筆電，那假如我們有了很多台機器然後呢？有什麼辦法是集結眾多機器的力量加速任務處理的呢？最近疫情嚴重在家讀MIT 6.824，對Google的AI大神Jeff Dean提出的MapReduce又認識了許多，之後許多知名的分散運算叢集都是基於這個概念，例如大家耳熟能詳的Hadoop和Spark，本篇文章將帶你深入淺出何謂MapReduce，下一篇則是以Go語言自幹一個MapReduce核心 ，有興趣可以先去我的github repo看。\n什麼是MapReduce 我們先想像有一個任務是要算出一篇很長的文章中字詞出現頻率，如果在同一台電腦我們會怎麼做呢？我們也許會遍歷整篇文章，並將字詞做紀錄，但若資料太大時這樣的方式並不可行，為了加速我們將檔案切分，並且產為經由Map function 產生 key-value pair如下：\nhello world -\u0026gt; {hello:1, world:1}\n接著將這些key-value pair 存成中介檔案後排序，Reduce function則根據key將所有value加總，如果以上流程在一台電腦上完成，會像下圖，前兩列中的A、B、C其實分別是Ａ:1、 B:1、C:1：\n  mapreduce in sequence, src: https://zhuanlan.zhihu.com/p/260752052\n  上面提到的任務如果在一個分散式的世界呢？Google提出的論文中有張重要的圖是這樣描述的：\n  mapreduce in distributed systems\n  MapReduce是一種軟體架構，由許多台機器組成，其中一台擔任Master，負責分派任務和處理來自Worker的RPC請求，其他則擔任Worker，負責處理使用者定義的Map和Reduce函式 也就是說只有可切分的大型任務才能應用MapReduce。而上面提到的例子如果以分散式系統實現，就會如下圖：\n  mapreduce in distributed systems, src: https://zhuanlan.zhihu.com/p/260752052\n  整個概念有點類似前陣子完成的Worker Pool，但是將thread改成分散於各個系統的Process。機器越多，系統擴展越困難，而MapReduce優雅的解決了這個問題，為了套用於許多不同的任務型態，mapreduce核心必須定義一致性的接口，這裏我們可以看到論文2.2提到的：\nmap (k1, v1) → list(k2, v2) reduce (k2, list(v2)) → list(v2) 在任務經過切分後，每個任務會交由一個mapper處理，而分配的機制則是誰有空誰就去處理(queue)。在word count的例子中，(k1, v1)代表切分後檔案名稱及內容，就是{filename, content}，而(k2, v2)則代表詞與頻率，也就是{hello: 1}。至於k2還有另一個用處，就是決定他要由哪個reducer處理，在產出中介檔案時就會依照mr-x-y命名，其中x代表map函數的index，y代表k2經過hash的值，這個值也同時是reducer的index。好的我知道許多人到這裡可能完全混亂，以下我用一張圖說明：","title":"[技術雜談] 你不能不知道的軟體架構--MapReduce (一)"},{"content":"背景 投遞時間落在2021/3-5月，大四即將畢業，實在對讀書沒什麼熱忱，雖然讀碩對職涯來說體質比較好，試著花了大約一個半月讀研究所(看github 的 contribution就知道有一個多月的空白qq)，但最後只有備取就認命開始找工作吧。想過將兵役提前，但今年因為疫情許多本來要出國的人或在國外的學生回台，造成等待兵役的人數增加，提前未必能在畢業後馬上進入軍營，於是有了先找工作慢慢等兵單的念頭，因此主要找的是可以給preoffer或以轉正為目標的實習為主。\nDcard (backend intern)   dcard logo\n  從大二就注意到Dcard的實習了，第一印象是年輕有活力的公司讓我很嚮往，但那時我的web知識匱乏連題目都看不懂(題目google就有，每年都一樣)，只好花時間多充實自己。大三時看到一位厲害的學長分享Dcard實習，覺得裡面的人對技術都相當有熱忱，也在gopher conf聽了Dcard backend精彩的演講，於是這次完成了作業決定嘗試看看。\n作業要求 Dcard 每天午夜都有大量使用者湧入抽卡，請設計一個 middleware：\n 限制每小時來自同一個 IP 的請求數量不得超過 1000 在 response headers 中加入剩餘的請求數量 (X-RateLimit-Remaining) 以及 rate limit 歸零的時間 (X-RateLimit-Reset) 如果超過限制的話就回傳 429 (Too Many Requests) 可以使用各種資料庫達成  實做方式 限定只能用golang或nodejs，資料庫並沒有限制。我是以golang寫，資料庫的部份以單線程為主的redis，使得單個transaction不用考慮race condition，加上存取快速高併發的特性，以及方便的ttl對這次的作業十分友善，因此選擇redis作為這次作業的資料庫。至於來自不同client的get和set可能造成race condition，我在github repo中有詳細說明我如何實做lock。\n一面 可以提早到hr會帶你參觀辦公室，由兩位backend面試後換hr面，先自我介紹約5-10分鐘，backend會針對以前的專案經驗追根究底的問，一定要對自己做過的專案非常熟悉，不然會和我一樣被問倒XD，加上以前在底層的計算機基礎知識不夠扎實，例如被問到為何mips要分成五個stage的pipeline來設計，當場腦袋空白只好說不知道，結果離開後馬上想到zzz。所以基礎也非常重要，面完就知道不會上了，期待蠻久的面試以失望落幕，果然自己還有許多基礎需要補足。和hr的面試就是一般的behavior問題，這個網路上有許多厲害的技巧分享，我就不多花篇幅了。\n結果 感謝信\nIBM (Associate developer) 官網投遞校園招募後一星期收到線上趣味測驗，趣味測驗結束就收到感謝信了。\n結果 感謝信\nAmazon Ring (SWE)   amazon ring\n  官網投遞校園招募後一星期收到線上測驗。\nOnsite Assignment 總共有三題，分別是easy, hard, medium。只能用四種語言: C, C++, Java, Python，職位的語言偏好也是前面的順序，第一次看到大科技公司對語言有偏好的，由於C-like的語言都不太熟，Java寫起來也不心安，雖然是最低順位的語言但也只能用Python了。\n答題過程 總共有90分鐘，第一題花了15-20分鐘，以easy來說蠻久的，讓我想花更多時間學好英文，不然閱讀速度太慢蠻虧的。第二題是比較複雜的背包問題，因為太久沒寫DP，花了很多時間還是沒寫出來，剩下20分鐘時放棄轉看第三題。結果第三題也是DP，比較基本的二維DP，看完還蠻有信心做出來，但後來和時間賽跑太緊張，index一直算錯還是沒有完成。雖然還沒收到信，但應該是沒什麼機會。\n更新後續 後來收到信說，這個職位比較偏向硬體的部份，OA僅能用C/C++完成，由於我是用python，於是HR問我要不要再用C like語言寫一次，但我實在對這樣的職務內容沒興趣，且對這兩個語言相當不熟，因此拒絕了機會。\n結果 reject\nGarmin (SRE intern)   garmin logo\n  在臉書社團看到的職位，實習期間為7-8月，算了一下和實習結束兵單應該差不多到，如果沒到可以去面更多不同的公司，看了許多關於garmin的薪資調查，以軟體來說應該算還不錯的，所以這個實習的目的除了接觸更多SRE實務經驗，有很大一部份是為了轉正而實習。1111投遞履歷後收到面試通知。\n面試 總部在汐止，但實習地點在林口，剛好離家不遠，不過當天騎到汐止的路上天色昏暗加上毛毛細雨，一路上看起來都相當荒涼，心中暗自慶幸自己不是在這上班。面試會先請你自我介紹，不會有白板題(其實還蠻期待的)。由於目前的實習也是SRE相關職務，所以講完自己的學經歷後就分享了自己對這職位的想法。我認為SRE對任何東西都需要了解一些，因為你不知道與自己對接的是怎樣的開發團隊，其實講到一點很有趣的是我大學跟風學了ML/AI的皮毛，但越是深入越是感覺無力以及渺小，之後便果斷放棄資料科學家一途。面試時我向面試官提到我有一個優勢是對ML有基礎的了解，剛好他們也有Data的團隊在嘗試DevOps，總覺得是命運使然，面試結束我蠻有信心會上的。\n 我相信努力不會騙人，也相信沒有路是白走的\n 結果 Offer Get\n小節 最後去了Garmin，但因為只是實習而已，所以實習尾聲如果有更多心得應該會更新，雖然是以轉正為目標，但還是會憧憬FANG的生活，無論是作夢或是野心，都會不斷訓練自己朝這個方向努力，尤其會更加強自己的英文\u0026gt;\u0026lt;如果文章有幫助到你，歡迎email和我聊聊，或是任何社交軟體私訊我我都會看到～\n","permalink":"http://nathan-tw.github.io/posts/intern/","summary":"背景 投遞時間落在2021/3-5月，大四即將畢業，實在對讀書沒什麼熱忱，雖然讀碩對職涯來說體質比較好，試著花了大約一個半月讀研究所(看github 的 contribution就知道有一個多月的空白qq)，但最後只有備取就認命開始找工作吧。想過將兵役提前，但今年因為疫情許多本來要出國的人或在國外的學生回台，造成等待兵役的人數增加，提前未必能在畢業後馬上進入軍營，於是有了先找工作慢慢等兵單的念頭，因此主要找的是可以給preoffer或以轉正為目標的實習為主。\nDcard (backend intern)   dcard logo\n  從大二就注意到Dcard的實習了，第一印象是年輕有活力的公司讓我很嚮往，但那時我的web知識匱乏連題目都看不懂(題目google就有，每年都一樣)，只好花時間多充實自己。大三時看到一位厲害的學長分享Dcard實習，覺得裡面的人對技術都相當有熱忱，也在gopher conf聽了Dcard backend精彩的演講，於是這次完成了作業決定嘗試看看。\n作業要求 Dcard 每天午夜都有大量使用者湧入抽卡，請設計一個 middleware：\n 限制每小時來自同一個 IP 的請求數量不得超過 1000 在 response headers 中加入剩餘的請求數量 (X-RateLimit-Remaining) 以及 rate limit 歸零的時間 (X-RateLimit-Reset) 如果超過限制的話就回傳 429 (Too Many Requests) 可以使用各種資料庫達成  實做方式 限定只能用golang或nodejs，資料庫並沒有限制。我是以golang寫，資料庫的部份以單線程為主的redis，使得單個transaction不用考慮race condition，加上存取快速高併發的特性，以及方便的ttl對這次的作業十分友善，因此選擇redis作為這次作業的資料庫。至於來自不同client的get和set可能造成race condition，我在github repo中有詳細說明我如何實做lock。\n一面 可以提早到hr會帶你參觀辦公室，由兩位backend面試後換hr面，先自我介紹約5-10分鐘，backend會針對以前的專案經驗追根究底的問，一定要對自己做過的專案非常熟悉，不然會和我一樣被問倒XD，加上以前在底層的計算機基礎知識不夠扎實，例如被問到為何mips要分成五個stage的pipeline來設計，當場腦袋空白只好說不知道，結果離開後馬上想到zzz。所以基礎也非常重要，面完就知道不會上了，期待蠻久的面試以失望落幕，果然自己還有許多基礎需要補足。和hr的面試就是一般的behavior問題，這個網路上有許多厲害的技巧分享，我就不多花篇幅了。\n結果 感謝信\nIBM (Associate developer) 官網投遞校園招募後一星期收到線上趣味測驗，趣味測驗結束就收到感謝信了。\n結果 感謝信\nAmazon Ring (SWE)   amazon ring\n  官網投遞校園招募後一星期收到線上測驗。\nOnsite Assignment 總共有三題，分別是easy, hard, medium。只能用四種語言: C, C++, Java, Python，職位的語言偏好也是前面的順序，第一次看到大科技公司對語言有偏好的，由於C-like的語言都不太熟，Java寫起來也不心安，雖然是最低順位的語言但也只能用Python了。","title":"[心得] 2021面試心得 Dcard/Garmin/Amazon/IBM"},{"content":"I am Hsuan-Yo Lin ","permalink":"http://nathan-tw.github.io/about/","summary":"I am Hsuan-Yo Lin ","title":"About"},{"content":"Star67 其實部落格的命名和星星完全沒有關係，大學時在新光路67號住了最久，也最喜歡在那生活的日子，所以部落格本想以此為名，不過如果取為New67或是Light67聽起來蠻蠢的，所以乾脆想成Star67aka星光路67號，而且在這的日子常常熬夜，五成是和朋友打傳說通霄，另外五成則是寫程式和書法到半夜停不下來所致，這些事的樂趣是日光還在時體會不到的，畢竟深夜才是適合苦惱的時候，而苦惱正是這些事精華之處。即使每天早上都不省人事，卻很享受這種報復性的作息，享受這種女友睡了，鄰居關燈了，剩下星光亮著的時光。\n重蹈覆轍  愛是讓你不重蹈覆轍，讓你生而為人 \u0026ndash; 羅于婷 \u0026lt;喜歡的話可以試穿\u0026gt;\n 其實大二大三時試著寫過medium，但一方面覺得自己的文章沒有質感，另一方面自己在技術上沒什麼可以分享或紀錄的，懷疑過現在的自己是否重蹈覆轍。直到最近聽到一首神曲走建國路回家但後座少ㄌ泥，我認為不管有沒有人看，寫的多爛，甚至什麼都沒學到，就去寫吧，寫的過程也許就是意義，一如那些星光陪伴的日子，寫下許多許多的字從來就不是為了讓誰滿意，而是我就想任性放肆的寫而已。\n  ","permalink":"http://nathan-tw.github.io/posts/star67/","summary":"Star67 其實部落格的命名和星星完全沒有關係，大學時在新光路67號住了最久，也最喜歡在那生活的日子，所以部落格本想以此為名，不過如果取為New67或是Light67聽起來蠻蠢的，所以乾脆想成Star67aka星光路67號，而且在這的日子常常熬夜，五成是和朋友打傳說通霄，另外五成則是寫程式和書法到半夜停不下來所致，這些事的樂趣是日光還在時體會不到的，畢竟深夜才是適合苦惱的時候，而苦惱正是這些事精華之處。即使每天早上都不省人事，卻很享受這種報復性的作息，享受這種女友睡了，鄰居關燈了，剩下星光亮著的時光。\n重蹈覆轍  愛是讓你不重蹈覆轍，讓你生而為人 \u0026ndash; 羅于婷 \u0026lt;喜歡的話可以試穿\u0026gt;\n 其實大二大三時試著寫過medium，但一方面覺得自己的文章沒有質感，另一方面自己在技術上沒什麼可以分享或紀錄的，懷疑過現在的自己是否重蹈覆轍。直到最近聽到一首神曲走建國路回家但後座少ㄌ泥，我認為不管有沒有人看，寫的多爛，甚至什麼都沒學到，就去寫吧，寫的過程也許就是意義，一如那些星光陪伴的日子，寫下許多許多的字從來就不是為了讓誰滿意，而是我就想任性放肆的寫而已。\n  ","title":"[生活碎念] 關於Star67"},{"content":"前陣子在準備Dcard實習的面試，看到別人的心得被問到如何以Golang實做一個Worker Pool，於是自己嘗試寫了一個。\n什麼是Worker Pool Worker Pool是一個以multithread組成的任務處理模型，producer產生許多任務，並交由workers並行處理這些任務，最後將任務結果蒐集起來，這樣的作法可以有效的運用電腦資源，並快速處理重複性高且獨立的作業。\nWorker Pool 設計 Type 定義 Worker Pool可以有一定數量的Workers(PoolSize)，並且可以指定一次處理的任務數量(tasksSize)，交由tasksChan送給worker，處理後再經由resultsChan存起來，待Results被呼叫時一一取出。而Task則是定義了id, error 和如何process的function。\ntype Task struct { Id int Err error f func() error } type WorkerPool struct { PoolSize int tasksSize int tasksChan chan Task resultsChan chan Task Results func() []Task } 功能實做 每一個任務處理完都會回傳error，如順利完成則回傳nil，至於任務如何執行則是依據新增任務時給定。\nfunc (task *Task) Do() error { return task.f() Worker Pool是利用buffered channel做任務的通道，所以可以經由Start將worker一一執行起來後，讓worker利用range的特性不斷監聽是否有任務可以做。\nfunc NewWorkerPool(tasks []Task, size int) *WorkerPool { tasksChan, resultsChan := make(chan Task, len(tasks)), make(chan Task, len(tasks)) for _, task := range tasks { tasksChan \u0026lt;- task } close(tasksChan) pool := \u0026amp;WorkerPool{ PoolSize: size, tasksSize: len(tasks), tasksChan: tasksChan, resultsChan: resultsChan, } pool.Results = pool.results return pool } func (pool *WorkerPool) Start() { for i := 0; i \u0026lt; pool.PoolSize; i++ { go pool.worker() } } func (pool *WorkerPool) worker() { for task := range pool.tasksChan { task.Err = task.Do() pool.resultsChan \u0026lt;- task } } func (wp *WorkerPool) results() []Task { tasks := make([]Task, wp.tasksSize) for i := 0; i \u0026lt; wp.tasksSize; i++ { tasks[i] = \u0026lt;-wp.resultsChan } return tasks } 小結 由於是第一次寫blog很多想法還沒結構化，寫文章也非常沒有系統性，如果有任何意見可以email (linnom987321@gmail.com)我。Worker Pool的作法其實有很多變化，我看過有人用Sync.Waitgroup做，也有人用mutex及unbuffered channel，我的實做及相關的連結在reference提供給大家。\nReference  https://github.com/nathan-tw/workerpool https://github.com/gammazero/workerpool https://github.com/xxjwxc/gowp  ","permalink":"http://nathan-tw.github.io/posts/worker-pool/","summary":"前陣子在準備Dcard實習的面試，看到別人的心得被問到如何以Golang實做一個Worker Pool，於是自己嘗試寫了一個。\n什麼是Worker Pool Worker Pool是一個以multithread組成的任務處理模型，producer產生許多任務，並交由workers並行處理這些任務，最後將任務結果蒐集起來，這樣的作法可以有效的運用電腦資源，並快速處理重複性高且獨立的作業。\nWorker Pool 設計 Type 定義 Worker Pool可以有一定數量的Workers(PoolSize)，並且可以指定一次處理的任務數量(tasksSize)，交由tasksChan送給worker，處理後再經由resultsChan存起來，待Results被呼叫時一一取出。而Task則是定義了id, error 和如何process的function。\ntype Task struct { Id int Err error f func() error } type WorkerPool struct { PoolSize int tasksSize int tasksChan chan Task resultsChan chan Task Results func() []Task } 功能實做 每一個任務處理完都會回傳error，如順利完成則回傳nil，至於任務如何執行則是依據新增任務時給定。\nfunc (task *Task) Do() error { return task.f() Worker Pool是利用buffered channel做任務的通道，所以可以經由Start將worker一一執行起來後，讓worker利用range的特性不斷監聽是否有任務可以做。\nfunc NewWorkerPool(tasks []Task, size int) *WorkerPool { tasksChan, resultsChan := make(chan Task, len(tasks)), make(chan Task, len(tasks)) for _, task := range tasks { tasksChan \u0026lt;- task } close(tasksChan) pool := \u0026amp;WorkerPool{ PoolSize: size, tasksSize: len(tasks), tasksChan: tasksChan, resultsChan: resultsChan, } pool.","title":"[技術雜談] Worker Pool併發處理模型"},{"content":"前言 fprintf(1, \u0026#34;hello world\\n\u0026#34;); 你可能好奇過 C 語言fprintf函數中，第一個參數1代表什麼，你也許聽過在 Unix 家族中Everything is a file，可是他究竟代表什麼意思呢？這篇我們想談談究竟什麼是file，以及作業系統如何達到i/o redirection。\n什麼是 file  Everything is a file\n 先從這句話開始講起，在類 Unix 的設計中，對所有 I/O 資源的近用都是透過資料流的方式，也就是透過 file system 定義的檔案描述檔(file descriptor)來傳輸，當開啟這些資源時就會回傳一個 file descriptor，代表的就是對這個 file 的控制，例如大家熟悉的 System call open:\nint open(char *file, int flags) 其中*file代表 path, flags代表 read/write，用法例如：\nfd = open(\u0026#34;/tmp/temp\u0026#34;, O_WRONLY|O_CREAT); 那為什麼是回傳一個 int 呢？其實那個 int 就是 file descriptor，因為每個 process 都有一個 fd (file descriptor) table，其中包含了fd flag以及open file entry，根據xv6 book對其的描述是：\n A file descriptor is a small integer representing a kernel-managed object that a process may read from or write to.\n 從上面的例子中可以理解，一個 process 可以透過打開一個檔案、資料夾、裝置或是製造一個pipe獲取 fd。當然我們也可以複製 fd，透過 system call dup可以達到等等會說的兩個process交換訊息。到這裡你可能會好奇，fd 是如何對資源進行控制的呢？事實上是透過剛剛提到的open file entry去訪問系統層級的open file table，再去inode查找自己需要的資源，以下用一張圖說明兩者的關係：\n  relationship between fd tables, open file table and inode table\n  由此可知，所有的file都指向一個底層的inode。以 xv6 系統為例，在kernel/stat.h是這樣定義inode的資訊：\n#define T_DIR 1 // Directory #define T_FILE 2 // File #define T_DEVICE 3 // Device  struct stat { int dev; // File system\u0026#39;s disk device  uint ino; // Inode number  short type; // Type of file  short nlink; // Number of links to file  uint64 size; // Size of file in bytes  }; 如果要獲取某個fd或是某個file所指向的inode資訊，我們可以使用以下兩個system call：\nint fstat(int fd, struct stat *st) int stat(char *file, struct stat *st) fstat是用fd獲取inode資訊，stat則是獲取某個path的inode資訊，而stat資訊則會被放入*st中，根據執行後的st.type，我們可以知道目前使用的資源是file, directory或是device。到這裡可能會開始困惑，為什麼要那麼麻煩呢？這樣的設計有什麼好處？下一篇我們會談到檔案系統的設計。\nI/O redirection 和 stdin, stdout, stderr int fprintf(FILE *stream, const char *format, ...) 如同有人可能會問不同process對同一個file做open會得到一樣的fd嗎？如同前面所說fd table是每個process各自擁有的，所以理論上會不同(或碰巧相同)，但有些情況卻不是碰巧，要了解原因必須先回答開頭的問題，fprintf中第一個參數1是什麼呢？相信很多人已經猜到，答案便是fd1，因為POSIX \u0026lt;unistd.h\u0026gt; 的定義是process在啟動時會先開啟三個stream: stdin, stdout, stderr，也就是我們剛剛說的fd0, 1, 2，預設是分別連接到用戶的終端設備，通常就是鍵盤和螢幕啦，如同下圖表示：\n  stdin, stdout, stderr\n  但也不是所有process都是這樣預設，因為如果經由一個parent processfork得到的child process，將會複製parent的fd table，這也是達成redirection的重點，process在安排fd時有一個原則：\n 以尚未被使用的最小int作為新的fd\n 所以我們可以透過抽換fd 0, 1, 2的值去完成redirection，舉我們常使用的cat \u0026lt; input.txtcommand來說，簡單的code其實是：\nchar *argv[2]; argv[0] = \u0026#34;cat\u0026#34;; argv[1] = 0; if (fork()==0) { // child process  close(0); // 把fd0關掉every  // 把指向input.txt資源的fd放在fd table 0的位置 (因為剛才0被關掉，所以最小是0)  open(\u0026#34;input.txt\u0026#34;, O_RDONLY); exec(\u0026#34;cat\u0026#34;, argv); } 上面的例子是在shell執行cat \u0026lt; input.txt的簡單範例，因為shell本身也是一個process，所以要透過fork去執行其他process，當fork回傳0代表示child process，這也是為什麼fork和exec要分成兩個system call，如果沒辦法抽換stdin, stdout和stderr，i/o就沒有彈性，也就是cat只能吃到shell的輸入，無法吃一個file作為輸入。既然可以child會複製parent，而我們又可以抽換他，因此偉大的pipe程式(|)也被人發明出來，以簡單的指令echo \u0026quot;hello world\u0026quot; | wc為例：\nint p[2]; char *argv[2]; argv[0] = \u0026#34;wc\u0026#34;; argv[1] = 0; pipe(p); // 創建一個pipe，將read/write fd分別放在p[0], p[1] if(fork()==0) { // child process  close(0); // 關掉從parent複製來的stdin  dup(p[0]); // 把pipe的read fd複製一份放到fd0  close(p[0]); // pipe用不到了便關掉  close(p[1]); exec(\u0026#34;/bin/wc\u0026#34;, argv); // wc會從stdin讀東西，目前stdin是剛剛設的read pipe } else { close(p[0]); write(p[1], \u0026#34;hello world\\n\u0026#34;, 12); // 把\u0026#34;hello world\u0026#34;寫進write pipe  close(p[1]); } 經由pipe串接不同的process，原本用法單調的程式就可以有許多組合技！下一篇寫file system對不同層的設計，讓我們如此方便的使用Everything is a file的概念。\n心得 因為很常使用linux，但對linux卻一直是一知半解，所以才決定寫這篇。但寫的時候很怕出錯，許多細節都更深入的查資料，還是有許多地方自己也看不懂的，如果有任何錯誤也歡迎在底下留言～\nReference  宅色夫老師筆記: https://hackmd.io/@sysprog/c-stream-io?type=view Redirection-in-bash: https://blog.hexrabbit.io/2019/10/22/Redirection-in-bash/ Ubuntu manual page: http://manpages.ubuntu.com/manpages/bionic/zh_TW/man3/stdin.3.html  ","permalink":"http://nathan-tw.github.io/posts/io-redirection/","summary":"前言 fprintf(1, \u0026#34;hello world\\n\u0026#34;); 你可能好奇過 C 語言fprintf函數中，第一個參數1代表什麼，你也許聽過在 Unix 家族中Everything is a file，可是他究竟代表什麼意思呢？這篇我們想談談究竟什麼是file，以及作業系統如何達到i/o redirection。\n什麼是 file  Everything is a file\n 先從這句話開始講起，在類 Unix 的設計中，對所有 I/O 資源的近用都是透過資料流的方式，也就是透過 file system 定義的檔案描述檔(file descriptor)來傳輸，當開啟這些資源時就會回傳一個 file descriptor，代表的就是對這個 file 的控制，例如大家熟悉的 System call open:\nint open(char *file, int flags) 其中*file代表 path, flags代表 read/write，用法例如：\nfd = open(\u0026#34;/tmp/temp\u0026#34;, O_WRONLY|O_CREAT); 那為什麼是回傳一個 int 呢？其實那個 int 就是 file descriptor，因為每個 process 都有一個 fd (file descriptor) table，其中包含了fd flag以及open file entry，根據xv6 book對其的描述是：\n A file descriptor is a small integer representing a kernel-managed object that a process may read from or write to.","title":"[技術雜談] 淺談 file descriptor 及 I/O Redirection"}]